{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ParserEvaluation.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/petervdonovan/CitationParser/blob/master/ParserEvaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"I-svER1i_Owv"},"source":["# Parser Evaluation\r\n","\r\n","Each citation parser that I test will have its dedicated section in this notebook."]},{"cell_type":"code","metadata":{"id":"x89UpgMSwU5z","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2750d530-9fd1-4711-eb1f-481d5968111f"},"source":["# Here, I simply import a couple of libraries and mount the Drive.\r\n","# There is nothing special to see here.\r\n","!pip install pickle5\r\n","\r\n","import pandas as pd\r\n","import numpy as np\r\n","import re\r\n","import random\r\n","import pickle5 as pickle\r\n","import time\r\n","from google.colab import drive"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: pickle5 in /usr/local/lib/python3.6/dist-packages (0.0.11)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OOppyhl5EasU","outputId":"c26f953b-d0ba-4456-d255-ac288bc5d9c7"},"source":["drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WKHBCq3MGrT3"},"source":["training_sets_dir = '/content/drive/My Drive/AWCA/Training/'\r\n","citation_tagging_dir = '/content/drive/My Drive/AWCA/Colab_notebooks/CitationTagging/'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YqZ9sqElt4fk"},"source":["## Loading Datasets\r\n","\r\n","I load and prepare three datasets:\r\n","1. The Zotero dataset, a non-public dataset curated by Dr. Anderson and prepared for our use by Jason Webb. This will be called `zotero`.\r\n","1. A preprocessed version of the Zotero dataset, the only difference being the `raw_text` column, which will have extraneous information such as capitalization and punctuation removed. This will be called `zotero_preproc`.\r\n","    * The primary purpose of this dataset is to verify that Jason Webb's results are being replicated correctly. For most purposes, I will use the non-preprocessed dataset because the objective is to be able to parse raw citation data, not pre-chewed citation data.\r\n","1. A small convenience sample of the OpenCitations dataset, a public, open-source dataset curated by [OpenCitations](https://opencitations.net/about). This will be called `occ`.\r\n","    * This dataset is far from ideal because it is not necessarily a diverse sample of the OpenCitations Corpus. For the time being, I had to take what I could get from their SPARQL endpoint, which seems unable to handle queries that are too complex or that return too many results.\r\n","    * This dataset is useful because it ensures _replicability_: Because the OpenCitations Corpus is completely public and accessible to anyone in the world, it will allow any interested individual to replicate our results.\r\n","\r\n","All datasets will have the following columns:\r\n","* 'raw_text'\r\n","* 'tags'\r\n","* 'contributors'\r\n","* 'title'\r\n","* 'year'\r\n","\r\n","All datasets will be split into train and test sets in this section. The train and test sets will be marked with the suffixes, `_train` and `_test`."]},{"cell_type":"markdown","metadata":{"id":"xP5R_fkNvFnX"},"source":["### Zotero Dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":106},"id":"ZOvVho7PvDoU","outputId":"7e219906-7f01-44b4-f7c2-d1866da7369d"},"source":["zotero = pd.read_csv(training_sets_dir + 'Labelled_Data_with_Dates.csv')\r\n","zotero = zotero.rename({'citations':'raw_text'}, axis=1).drop('Unnamed: 0', axis=1)\r\n","zotero.head(2)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>raw_text</th>\n","      <th>tags</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Stieglitz, R. R. (2001b). Ebla and the Gods of...</td>\n","      <td>A A A D T T T T T T O O O O O O O O O O O O O ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Felsner, P. (2001, May 1). Lecture on Recent E...</td>\n","      <td>A A D O O T T T T T T</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            raw_text                                               tags\n","0  Stieglitz, R. R. (2001b). Ebla and the Gods of...  A A A D T T T T T T O O O O O O O O O O O O O ...\n","1  Felsner, P. (2001, May 1). Lecture on Recent E...                             A A D O O T T T T T T "]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"cZdb7A9AzycE"},"source":["Here, I define a function for extracting words from one string that correspond to certain tags in another string. This is a specialized function intended specifically for expanding out the information in the saved CSV file."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"Sl_mN3Bs0ukG","outputId":"a9dc7f78-78ac-4ba8-aa9c-d78e9a2b795c"},"source":["def extract_from_raw(raw, tags, desired_tag):\r\n","  \"\"\"Returns the substring from CITATION corresponding a DESIRED_TAG in the\r\n","  TAGS string.\r\n","  \"\"\"\r\n","  ret = ''\r\n","  for word, tag in zip(raw.split(), tags.split()):\r\n","    if tag == desired_tag:\r\n","      ret += word + ' '\r\n","  return ret[:-1]\r\n","extract_from_raw(zotero.raw_text[0], zotero.tags[0], 'A')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Stieglitz, R. R.'"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"AM5kwczp3IBu"},"source":["def expand_raw_and_tags(df):\r\n","  \"\"\"Converts a DataFrame containing just raw text and tags to a new DataFrame\r\n","  that also has the columns 'contributors', 'title', and 'year'. (This is a\r\n","  mutative operation -- it acts via a side effect.)\r\n","  \"\"\"\r\n","  for col_name, tag in [('contributors', 'A'), ('title', 'T'), ('year', 'D')]:\r\n","    df[col_name] = df.apply(\r\n","        lambda row: extract_from_raw(row.raw_text, row.tags, tag),\r\n","        axis=1)\r\n","  return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":140},"id":"9C9FUsmCJjLw","outputId":"a9b0bbab-d864-4433-b0f5-6ae130d6e9d1"},"source":["expand_raw_and_tags(zotero)\r\n","zotero.head(2)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>raw_text</th>\n","      <th>tags</th>\n","      <th>contributors</th>\n","      <th>title</th>\n","      <th>year</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Stieglitz, R. R. (2001b). Ebla and the Gods of...</td>\n","      <td>A A A D T T T T T T O O O O O O O O O O O O O ...</td>\n","      <td>Stieglitz, R. R.</td>\n","      <td>Ebla and the Gods of Canaan.</td>\n","      <td>(2001b).</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Felsner, P. (2001, May 1). Lecture on Recent E...</td>\n","      <td>A A D O O T T T T T T</td>\n","      <td>Felsner, P.</td>\n","      <td>Lecture on Recent Excavations at Tell-Mishrife...</td>\n","      <td>(2001,</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            raw_text  ...      year\n","0  Stieglitz, R. R. (2001b). Ebla and the Gods of...  ...  (2001b).\n","1  Felsner, P. (2001, May 1). Lecture on Recent E...  ...    (2001,\n","\n","[2 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"MHnXGGj87i7D"},"source":["As a final bit of housekeeping, I wish to clean up the 'year' column. I observe that a considerable number of entries tagged as 'D' are actually cities or pagination markers (i.e., 'pp.')."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":140},"id":"53copK3T9m6p","outputId":"1afa3ceb-b205-40f5-d884-1a37df916390"},"source":["zotero.head(2)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>raw_text</th>\n","      <th>tags</th>\n","      <th>contributors</th>\n","      <th>title</th>\n","      <th>year</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Stieglitz, R. R. (2001b). Ebla and the Gods of...</td>\n","      <td>A A A D T T T T T T O O O O O O O O O O O O O ...</td>\n","      <td>Stieglitz, R. R.</td>\n","      <td>Ebla and the Gods of Canaan.</td>\n","      <td>(2001b).</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Felsner, P. (2001, May 1). Lecture on Recent E...</td>\n","      <td>A A D O O T T T T T T</td>\n","      <td>Felsner, P.</td>\n","      <td>Lecture on Recent Excavations at Tell-Mishrife...</td>\n","      <td>(2001,</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            raw_text  ...      year\n","0  Stieglitz, R. R. (2001b). Ebla and the Gods of...  ...  (2001b).\n","1  Felsner, P. (2001, May 1). Lecture on Recent E...  ...    (2001,\n","\n","[2 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"weHafBfP70jg","outputId":"5fbc9fe9-a35b-4a7d-c2a6-4c484a0f160c"},"source":["errors = []\r\n","def get_year(text):\r\n","  year = re.search('\\d\\d\\d\\d', text)\r\n","  if year:\r\n","    return int(year.group(0))\r\n","  else:\r\n","    errors.append(text)\r\n","    return None\r\n","zotero.year = zotero.apply(lambda row: get_year(row.year), axis=1)\r\n","print('There were {} substrings incorrectly tagged as dates. Randomly chosen\\n'\r\n","      'examples include the following:\\n{}'.format(\r\n","          len(errors), str(random.sample(errors, 10))))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["There were 3875 substrings incorrectly tagged as dates. Randomly chosen\n","examples include the following:\n","['', 'Chicago.', '(Ugaritica Tel', '', 'Copenhagen.', 'Berlin.', 'Leiden.', 'Ghent.', '', 'Paris.']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HYD-Y6xnALl0","outputId":"7b6eedd5-4fd0-4b22-be93-010b42e0c1df"},"source":["print('We now have a dataset called \"zotero\" with {} rows.'.format(len(zotero.index)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["We now have a dataset called \"zotero\" with 66985 rows.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UVa3yY0kFdbZ"},"source":["I conclude by splitting the dataset into train and test sets. I use a 15:85 train-test split, as is done in the \"[Accuracy](https://colab.research.google.com/drive/1abvov19jS3A4r_MpGbTOyV8LvJmiTj4F#scrollTo=S_Y5yeGdWYHG)\" notebook."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":157},"id":"2vkyBfRiF2jc","outputId":"b91aab32-237a-4d2e-95ad-d636048b3935"},"source":["zotero_train = zotero.sample(frac=0.15)\r\n","zotero_test = zotero.drop(zotero_train.index)\r\n","zotero_train.head(2)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>raw_text</th>\n","      <th>tags</th>\n","      <th>contributors</th>\n","      <th>title</th>\n","      <th>year</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>58688</th>\n","      <td>Mayrhofer, Manfred. Nachlese altpersischer Ins...</td>\n","      <td>A O T T T T T T T T T T T T T T T T T T T T T ...</td>\n","      <td>Mayrhofer,</td>\n","      <td>Nachlese altpersischer Inschriften. Zu ̈uberse...</td>\n","      <td>1978.0</td>\n","    </tr>\n","    <tr>\n","      <th>39152</th>\n","      <td>COUROYER, B. \"LES AAMOU LES AAMOU-HYKSOS ET LE...</td>\n","      <td>A O T T T T T T T D O O</td>\n","      <td>COUROYER,</td>\n","      <td>\"LES AAMOU LES AAMOU-HYKSOS ET LES CANANEO-PHE...</td>\n","      <td>1974.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                raw_text  ...    year\n","58688  Mayrhofer, Manfred. Nachlese altpersischer Ins...  ...  1978.0\n","39152  COUROYER, B. \"LES AAMOU LES AAMOU-HYKSOS ET LE...  ...  1974.0\n","\n","[2 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"0PLf23f2Afjh"},"source":["### Preprocessed Zotero Dataset\r\n","\r\n","Jason Webb used a preprocessed version of the Zotero dataset with his BiLSTM model. I load the dataset below to verify that his results are being replicated here correctly."]},{"cell_type":"code","metadata":{"id":"v4S1OhEyEURi"},"source":["with open(training_sets_dir + 'BiLSTM_train.pickle', 'rb') as dbfile:\r\n","  zotero_preproc_train_original = pickle.load(dbfile)\r\n","with open(training_sets_dir + 'BiLSTM_test.pickle',  'rb') as dbfile:\r\n","  zotero_preproc_test_original  = pickle.load(dbfile)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aCqV4-6rHsqj"},"source":["These loaded sets are formatted as lists of ordered pairs. Each ordered pair contains a list of words from the raw text and a list of tags. This is nice, but it is nonstandard, and so it makes it hard to compare models side-by-side. Below I define two methods to help with converting between the two formats."]},{"cell_type":"code","metadata":{"id":"nsTdvmPeIf57"},"source":["def listpairlist_to_df(lpl):\r\n","  raw_texts = []\r\n","  tags = []\r\n","  for raw, tag in lpl:\r\n","    raw_texts.append(' '.join(raw))\r\n","    tags.append(' '.join(tag))\r\n","  return pd.DataFrame(data={\r\n","      'raw_text': raw_texts,\r\n","      'tags': tags\r\n","  })\r\n","def df_to_listpairlist(df):\r\n","  lpl = []\r\n","  for raw, tag in zip(df.raw_text, df.tags):\r\n","    lpl.append((raw.split(), tag.split()))\r\n","  return lpl"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LC3wsW9JNwqk"},"source":["Here, I verify that the functions defined above are inverse functions of each other."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u5Ui8pn_HiQ8","outputId":"8bacede5-8f76-4baa-e10b-f86de59526a5"},"source":["inverse_inverse = df_to_listpairlist(listpairlist_to_df(zotero_preproc_train_original))\r\n","diff = [item for idx, item in enumerate(zotero_preproc_train_original) if \r\n","        inverse_inverse[idx] != item\r\n","       ]\r\n","print('There are {} elements in a list that are not in the output of a function'\r\n","      ' composed\\nwith its inverse applied to that list. Furthermore, the '\r\n","      'difference in the lengths\\nof the two lists is {}.\\nThe two lists {} '\r\n","      'equal, and the first {} the second.'.format(\r\n","          len(diff),\r\n","          len(zotero_preproc_train_original) - len(inverse_inverse),\r\n","          'are' if zotero_preproc_train_original == inverse_inverse else 'are not',\r\n","          'is'  if zotero_preproc_train_original is inverse_inverse else 'is not'\r\n","      ))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["There are 0 elements in a list that are not in the output of a function composed\n","with its inverse applied to that list. Furthermore, the difference in the lengths\n","of the two lists is 0.\n","The two lists are equal, and the first is not the second.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":140},"id":"1sown-49QJ0Y","outputId":"81230aa6-5897-433a-c9b7-fd95cef90b70"},"source":["zotero_preproc_train = listpairlist_to_df(zotero_preproc_train_original)\r\n","expand_raw_and_tags(zotero_preproc_train)\r\n","zotero_preproc_test = listpairlist_to_df(zotero_preproc_test_original)\r\n","expand_raw_and_tags(zotero_preproc_test)\r\n","zotero_preproc_train.head(2)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>raw_text</th>\n","      <th>tags</th>\n","      <th>contributors</th>\n","      <th>title</th>\n","      <th>year</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>cole s &amp; gasche h fourdigitnum second-and firs...</td>\n","      <td>O A O A A D T T T T T T T O O O O O O O O O O ...</td>\n","      <td>s gasche h</td>\n","      <td>second-and first-millennium bc rivers in north...</td>\n","      <td>fourdigitnum</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>cooper jerrold fourdigitnum buddies in babylon...</td>\n","      <td>A A D T T T T T T T T O O O O O O O O O O O O ...</td>\n","      <td>cooper jerrold</td>\n","      <td>buddies in babylonia gilgamesh enkidu and meso...</td>\n","      <td>fourdigitnum</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            raw_text  ...          year\n","0  cole s & gasche h fourdigitnum second-and firs...  ...  fourdigitnum\n","1  cooper jerrold fourdigitnum buddies in babylon...  ...  fourdigitnum\n","\n","[2 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"yHgqc5kXJ22H"},"source":["### OpenCitations Dataset\r\n","\r\n","Here, I load a 5000-row dataset consisting of bibliographic entries from the OpenCitations corpus. This dataset was downloaded from OpenCitations using [this notebook](https://github.com/petervdonovan/CitationParser/blob/master/datasets/ccc_dataset.ipynb). It is worth noting that with sufficient time, this dataset could have been made ten times larger; however, as a proof of concept, I kept this one to 5000 rows."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QVNEoHG7KJPG","outputId":"8f5967cf-95be-495f-c430-d65a07eae893"},"source":["%cd /content/drive/My Drive/AWCA/Colab_notebooks/CitationTagging/ParserEvaluation\r\n","!rm -r CitationParser\r\n","!git clone https://github.com/petervdonovan/CitationParser.git\r\n","%cd CitationParser/datasets\r\n","!ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/.shortcut-targets-by-id/1W2EROe2FItlaK99U-WY_qaBOc2UD_LI0/AWCA/Colab_notebooks/CitationTagging/ParserEvaluation\n","Cloning into 'CitationParser'...\n","remote: Enumerating objects: 23, done.\u001b[K\n","remote: Counting objects: 100% (23/23), done.\u001b[K\n","remote: Compressing objects: 100% (17/17), done.\u001b[K\n","remote: Total 23 (delta 1), reused 20 (delta 1), pack-reused 0\u001b[K\n","Unpacking objects: 100% (23/23), done.\n","/content/drive/.shortcut-targets-by-id/1W2EROe2FItlaK99U-WY_qaBOc2UD_LI0/AWCA/Colab_notebooks/CitationTagging/ParserEvaluation/CitationParser/datasets\n","ccc_dataset.ipynb  dataset19183.pickle\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":140},"id":"_5lTkOuKKdgB","outputId":"1e885f78-5691-48b3-bdae-b371c4298f9b"},"source":["with open('dataset19183.pickle', 'rb') as dbfile:\r\n","  occ = pickle.load(dbfile)\r\n","occ.head(2)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>raw_text</th>\n","      <th>tags</th>\n","      <th>contributors</th>\n","      <th>title</th>\n","      <th>year</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Holowka, D, Wensel, T, Baird, B. A nanosecond ...</td>\n","      <td>A A A A A A A O O O O O O O O O O O O O D O O ...</td>\n","      <td>Holowka, David; Wensel, Theodore; Baird, Barbara</td>\n","      <td>A Nanosecond Fluorescence Depolarization Study...</td>\n","      <td>1990.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Arias-Salgado E. G. et al. Src kinase activati...</td>\n","      <td>A A A O O O O O O O O O O O O O O O O O O O O ...</td>\n","      <td>Arias Salgado, E. G.; Lizano, S.; Sarkar, S.; ...</td>\n","      <td>Src Kinase Activation By Direct Interaction Wi...</td>\n","      <td>2003.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            raw_text  ...    year\n","0  Holowka, D, Wensel, T, Baird, B. A nanosecond ...  ...  1990.0\n","1  Arias-Salgado E. G. et al. Src kinase activati...  ...  2003.0\n","\n","[2 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S3hPM64wLk7F","outputId":"d480534a-9535-483d-9dd1-c215840825bc"},"source":["print('Originally the dataframe \"occ\" had {} rows'.format(len(occ.index)), end='')\r\n","occ = occ.dropna()\r\n","print(', but after dropping null values it now has {} rows.'.format(len(occ.index)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Originally the dataframe \"occ\" had 5000 rows, but after dropping null values it now has 4783 rows.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"U5tchWJOKKyD"},"source":["# TestRunner\r\n","\r\n","Here, I define a couple of classes that are necessary for running tests. These classes determine what metrics I use to evaluate parser performance."]},{"cell_type":"markdown","metadata":{"id":"MXM2wXEsB1Ta"},"source":["## TestRunner\r\n","\r\n","Every Testable parser can be given to a TestRunner instance to permit the computation of performance metrics."]},{"cell_type":"markdown","metadata":{"id":"KNq0zv2ZWo5J"},"source":["By the way, four of the metrics that I borrowed from the notebook [Accuracy](https://colab.research.google.com/drive/1D7i5pLgqEsrLG3PdwiZKcSMOfafWvztl) have a quirk: A tagger that labels everything with a certain tag can have 100% accuracy for that specific tag, even though its output provides zero information. This is why the next re-implementation of this might involve [F-scores](https://en.wikipedia.org/wiki/F-score) instead."]},{"cell_type":"code","metadata":{"id":"cr0ILtE2CIgj"},"source":["class TestRunner:\r\n","  def __init__(self, model, test_set):\r\n","    \"\"\"Initializes the TestRunner with a trained MODEL that is Testable and a\r\n","    TEST_SET, which is a DataFrame that has the columns specified in the above\r\n","    section, \"Loading Datasets.\"\r\n","    \"\"\"\r\n","    self.model = model\r\n","    self.test_set = test_set\r\n","    # self.predictions will be a DataFrame of the same format as TEST_SET.\r\n","    # However, it is set to None here because it will be computed lazily.\r\n","    self.predictions = None\r\n","  def get_predictions(self):\r\n","    if self.predictions is None:\r\n","      self.predictions = self.model.predict(self.test_set.raw_text)\r\n","    return self.predictions\r\n","  def get_predict_tags(self):\r\n","    return self.get_predictions().tags\r\n","  def token_tagging_accuracy(self, boot=False):\r\n","    \"\"\"Returns the proportion of tokens in the test set that are correctly\r\n","    tagged.\r\n","    \"\"\"\r\n","    tokens_correct = 0\r\n","    tokens_total = 0\r\n","    sample = zip(self.get_predict_tags(), self.test_set.tags)\r\n","    if boot:\r\n","      sample = list(sample)\r\n","      sample = random.choices(sample, k=len(sample))\r\n","    for predict, actual in sample:\r\n","      for token_predict, token_actual in zip(predict.split(), actual.split()):\r\n","        tokens_total += 1\r\n","        if token_predict == token_actual:\r\n","          tokens_correct += 1\r\n","    return tokens_correct / tokens_total\r\n","  def _semantic_unit_tagging_accuracy(self, tag, boot=False):\r\n","    \"\"\"Returns the proportion of citations for which every word corresponding to\r\n","    TAG is correctly tagged as such. Note: This method returns 100%  (1.0) if\r\n","    every single word is tagged as TAG because it does not account for false\r\n","    positives.\r\n","    \"\"\"\r\n","    correct = 0\r\n","    total = 0\r\n","    sample = zip(self.get_predict_tags(), self.test_set.tags)\r\n","    if boot:\r\n","      sample = list(sample)\r\n","      sample = random.choices(sample, k=len(sample))\r\n","    for predict, actual in sample:\r\n","      total += 1\r\n","      if all([\r\n","              token_actual != tag or token_predict == token_actual\r\n","              for token_predict, token_actual in\r\n","              zip(predict.split(), actual.split())]):\r\n","        correct += 1\r\n","    return correct / total\r\n","  def _column_accuracy(self, column, boot=False, comparator=None):\r\n","    \"\"\"Returns the proportion of citations that have a non-null value in the\r\n","    COLUMN column for which the value in that column is predicted correctly.\r\n","    COMPARATOR (optional): The function that returns a boolean depending on\r\n","        whether two values are equal (or approximately equal)\r\n","    \"\"\"\r\n","    selected_test = self.test_set[column].dropna()\r\n","    selected_pred = self.get_predictions().loc[selected_test.index,column]\r\n","    if boot:\r\n","      idx = random.choices(selected_test.index, k=len(selected_test.index))\r\n","      selected_test = selected_test.loc[idx]\r\n","      selected_pred = selected_pred.loc[idx]\r\n","    if comparator is None:\r\n","      return sum(selected_test == selected_pred) / len(selected_test)\r\n","    return sum(\r\n","        comparator(test, pred)\r\n","        for test, pred in zip(selected_test, selected_pred)\r\n","        ) / len(selected_test)\r\n","  def report(self, n_boots=0):\r\n","    \"\"\"Side effect: Prints out a report on the performance of the model and uses\r\n","    N_BOOTS bootstrap samples to compute confidence intervals. If N_BOOTS==0,\r\n","    then no confidence intervals are computed.\r\n","    \"\"\"\r\n","    t0 = time.time()\r\n","    self.get_predictions()\r\n","    print('Time to get predictions: {:.4f} seconds.'.format(time.time() - t0))\r\n","    def report_metric(msg, method):\r\n","      print('{}: {:.4f}.'.format(\r\n","          msg, method(False)\r\n","      ) + ('' if not n_boots else ' 95% CI: ({:.4f}, {:.4f})'.format(\r\n","          *boot(lambda: method(True), n_boots, 0.95)\r\n","      )))\r\n","    report_metric('Proportion of tokens correctly tagged',\r\n","                  self.token_tagging_accuracy)\r\n","    report_metric('Proportion of citations for which every word corresponding\\n'\r\n","                  'to a contributor\\'s name is correctly tagged as such',\r\n","                  lambda boot: self._semantic_unit_tagging_accuracy('A', boot))\r\n","    report_metric('Proportion of citations for which every word corresponding\\n'\r\n","                  'to a part of a title is correctly tagged as such',\r\n","                  lambda boot: self._semantic_unit_tagging_accuracy('T', boot))\r\n","    report_metric('Proportion of citations for which every word corresponding\\n'\r\n","                  'to the year of publication is correctly tagged as such',\r\n","                  lambda boot: self._semantic_unit_tagging_accuracy('D', boot))\r\n","    for column in self.test_set.drop('raw_text', axis=1).columns:\r\n","      report_metric('Proportion of citations that have a non-null value in the\\n'\r\n","                    '{} column for which the value in that column is predicted\\n'\r\n","                    'correctly'.format(column),\r\n","                    lambda boot: self._column_accuracy(column, boot))\r\n","    report_metric('Proportion of titles predicted approximately correctly',\r\n","                  lambda boot: self._column_accuracy(\r\n","                      'title', boot, str_approx_equal))\r\n","def boot(f, n, confidence):\r\n","  \"\"\"Returns a confidence interval for the result of calling F using N trials.\r\n","  \"\"\"\r\n","  results = np.zeros(n)\r\n","  for i in range(n):\r\n","    results[i] = f()\r\n","  alpha = 1 - confidence\r\n","  return (np.percentile(results, 100*alpha/2),\r\n","          np.percentile(results, 100*(1-alpha/2)))\r\n","def str_approx_equal(a, b):\r\n","  \"\"\"Returns whether or not the lowercase of A with only spaces and word\r\n","  characters equals the lowercase of B with only spaces and word characters.\r\n","  \"\"\"\r\n","  if a is None or b is None:\r\n","    return False\r\n","  assert isinstance(a, str) and isinstance(b, str), \\\r\n","      'Can only compare strings but got {} and {}'.format(a, b)\r\n","  return re.sub(r'[^\\w ]+', '', a).lower() == re.sub(r'[^\\w ]+', '', b).lower()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xszIfsbL9jnk"},"source":["## A Testable Interface\r\n","\r\n","For testing purposes, I want every implementation of a citation parser to have at least some of the same methods. This is necessary for comparison. However, because many other parts of our project are still under development, ideas about the functionality that is truly necessary may continue to evolve. In short, we want flexibility for development but uniform behavior for testing.\r\n","\r\n","To resolve this contradiction, I draw the following conclusion: Citation parsers should not necessarily implement the same abstract class. However, we should have wrapper classes for them that all implement the same abstract class."]},{"cell_type":"code","metadata":{"id":"n1db5Vd7BTfj"},"source":["from abc import ABC, abstractmethod\r\n","\r\n","class Testable(ABC):\r\n","  @abstractmethod\r\n","  def predict(self):\r\n","    \"\"\"Returns a Pandas DataFrame with columns that directly correspond to the\r\n","    columns of TEST, for the purpose of comparison.\r\n","    \"\"\"\r\n","    pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gbmyaOpxLT1-"},"source":["## Testing Parsers\r\n","\r\n","This is the section where I begin to run accuracy tests on different parsers."]},{"cell_type":"markdown","metadata":{"id":"F1IoVOe251zl"},"source":["### A Regex-Based Parser\r\n","\r\n","Here, I load and test a parser that uses regular expressions. This implementation is inherently messy: It uses hardcoded rules to identify parts of a citation. However, it is tested below because it is convenient, fast, and simple, and because it provides a reasonable baseline performance level that other models should outperform. Any more advanced model is probably not good enough to publicly use unless it can outperform the model below."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VMEiADuek2lO","outputId":"382be979-faa2-48fb-d5a4-68f84d137938"},"source":["%cd /content/drive/My Drive/AWCA/Colab_notebooks/CitationTagging/ParserEvaluation\r\n","!rm -r CitationParser0\r\n","!git clone https://github.com/petervdonovan/CitationParser0.git\r\n","%cd CitationParser0\r\n","!ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/.shortcut-targets-by-id/1W2EROe2FItlaK99U-WY_qaBOc2UD_LI0/AWCA/Colab_notebooks/CitationTagging/ParserEvaluation\n","Cloning into 'CitationParser0'...\n","remote: Enumerating objects: 33, done.\u001b[K\n","remote: Counting objects: 100% (33/33), done.\u001b[K\n","remote: Compressing objects: 100% (21/21), done.\u001b[K\n","remote: Total 33 (delta 9), reused 31 (delta 7), pack-reused 0\u001b[K\n","Unpacking objects: 100% (33/33), done.\n","/content/drive/.shortcut-targets-by-id/1W2EROe2FItlaK99U-WY_qaBOc2UD_LI0/AWCA/Colab_notebooks/CitationTagging/ParserEvaluation/CitationParser0\n","Citations  development_set.py  People  README.md  Utils\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vvRxZdzVM0_U"},"source":["Because the original `Citation` class that I uploaded above generates titles and author names, not tags, the wrapper class must convert the output of the `Citation` class into tags in order for its performance to be evaluated."]},{"cell_type":"code","metadata":{"id":"a0iO_OXTx0gN"},"source":["import Citations.Citation as cp0\r\n","class RegexParserWrapper(Testable):\r\n","  def predict(self, test):\r\n","    tags = []\r\n","    contributors = []\r\n","    title = []\r\n","    year = []\r\n","    for raw in test:\r\n","      citation = cp0.Citation(raw)\r\n","      current_contribs = citation.getNameList()\r\n","      current_year = citation.getYear()\r\n","      current_title = citation.getTitle()\r\n","      contributors.append(current_contribs)\r\n","      year.append(current_year)\r\n","      title.append(current_title)\r\n","      # In the next several lines, I use the knowledge that the parser has\r\n","      # provided to convert the raw text into a tag.\r\n","      tag = ''\r\n","      while raw != '':\r\n","        next_space = raw.find(' ') % len(raw)\r\n","        if current_contribs and raw.find(current_contribs) == 0:\r\n","          tag += 'A ' * len(current_contribs.split())\r\n","          raw = raw[len(current_contribs):]\r\n","        elif current_year and raw.find(str(current_year)) % len(raw) < next_space:\r\n","          tag += 'D '\r\n","          # Go to the next word, or to the end if no spaces remain.\r\n","          raw = raw[raw.find(' ') % len(raw) + 1:]\r\n","        elif current_title and raw.find(current_title) == 0:\r\n","          tag += 'T ' * len(current_title.split())\r\n","          raw = raw[len(current_title):]\r\n","        else:\r\n","          tag += 'O '\r\n","          # Go to the next word, or to the end if no spaces remain.\r\n","          raw = raw[next_space + 1:]\r\n","        raw = raw.strip()\r\n","      tags.append(tag)\r\n","    return pd.DataFrame({'raw_text': test,\r\n","                         'tags': tags,\r\n","                         'contributors': contributors,\r\n","                         'title': title,\r\n","                         'year': year})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HjxR-yR-Mtey"},"source":["Here I test the regex-based parser on the **Zotero test dataset**."]},{"cell_type":"code","metadata":{"id":"UkcrKTb8brVL","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3e0fd2ea-5030-4910-8866-239ed5d252cc"},"source":["rpw = RegexParserWrapper()\r\n","TestRunner(rpw, zotero_test).report() # Please pass a big number to this function (like,\r\n","                           # 500) if you want useful results (i.e., a confidence\r\n","                           # interval). Otherwise, just pass 0 or nothing so\r\n","                           # that it will run quickly."],"execution_count":null,"outputs":[{"output_type":"stream","text":["Time to get predictions: 7.6095 seconds.\n","Proportion of tokens correctly tagged: 0.7118.\n","Proportion of citations for which every word corresponding\n","to a contributor's name is correctly tagged as such: 0.8672.\n","Proportion of citations for which every word corresponding\n","to a part of a title is correctly tagged as such: 0.6246.\n","Proportion of citations for which every word corresponding\n","to the year of publication is correctly tagged as such: 0.8327.\n","Proportion of citations that have a non-null value in the\n","tags column for which the value in that column is predicted\n","correctly: 0.0447.\n","Proportion of citations that have a non-null value in the\n","contributors column for which the value in that column is predicted\n","correctly: 0.0550.\n","Proportion of citations that have a non-null value in the\n","title column for which the value in that column is predicted\n","correctly: 0.4351.\n","Proportion of citations that have a non-null value in the\n","year column for which the value in that column is predicted\n","correctly: 0.9420.\n","Proportion of titles predicted approximately correctly: 0.4404.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SN3jh1_jMjud"},"source":["Here I  test the regex-based parser on the **OCC dataset**."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FIRByo_jNM7N","outputId":"3c9e9249-3f35-47ad-a939-d683cb3a3fb3"},"source":["regex_test_runner = TestRunner(rpw, occ)\r\n","regex_test_runner.report(1000) # Please pass a big number to this function (like,\r\n","                           # 500) if you want useful results (i.e., a confidence\r\n","                           # interval). Otherwise, just pass 0 or nothing so\r\n","                           # that it will run quickly."],"execution_count":null,"outputs":[{"output_type":"stream","text":["Time to get predictions: 0.9780 seconds.\n","Proportion of tokens correctly tagged: 0.6340. 95% CI: (0.6270, 0.6410)\n","Proportion of citations for which every word corresponding\n","to a contributor's name is correctly tagged as such: 0.6939. 95% CI: (0.6814, 0.7067)\n","Proportion of citations for which every word corresponding\n","to a part of a title is correctly tagged as such: 0.6715. 95% CI: (0.6584, 0.6851)\n","Proportion of citations for which every word corresponding\n","to the year of publication is correctly tagged as such: 0.4010. 95% CI: (0.3872, 0.4144)\n","Proportion of citations that have a non-null value in the\n","tags column for which the value in that column is predicted\n","correctly: 0.0100. 95% CI: (0.0071, 0.0130)\n","Proportion of citations that have a non-null value in the\n","contributors column for which the value in that column is predicted\n","correctly: 0.0000. 95% CI: (0.0000, 0.0000)\n","Proportion of citations that have a non-null value in the\n","title column for which the value in that column is predicted\n","correctly: 0.0000. 95% CI: (0.0000, 0.0000)\n","Proportion of citations that have a non-null value in the\n","year column for which the value in that column is predicted\n","correctly: 0.8953. 95% CI: (0.8863, 0.9036)\n","Proportion of titles predicted approximately correctly: 0.3174. 95% CI: (0.3036, 0.3301)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CgSMJUx0ninB"},"source":["### BiLSTM Parser\r\n","\r\n","The parser loaded below is the BiLSTM model that was created, trained, and saved in the drive by Jason Webb."]},{"cell_type":"markdown","metadata":{"id":"uosgm6R5uR8W"},"source":["First, some setup. It is necessary to load a few objects and define a few functions and mappings that are copied from the \"Accuracy\" notebook.\r\n","\r\n","Note: It is necessary to downgrade Pytorch to an earlier version such as 1.2.0 to get the LSTM model working. If you have not done that in your current Google Drive session, then it is necessary to run the cell below."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gdF01y_yqag_","outputId":"c790e9e9-127a-4faf-f198-a097505a2065"},"source":["!pip install torch==1.2.0"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch==1.2.0 in /usr/local/lib/python3.6/dist-packages (1.2.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.2.0) (1.19.5)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xt5FhuHkNM8C"},"source":["The following code is necessary to convert between the input/output of the `BiLSTMTagger` class (numbers) and input/output that humans can understand (words). For some reason, such functionality is not built into the original class, which I did not wish to modify."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i1ImZHbguQ-5","outputId":"037a0f7a-2e3f-4c4e-e3c0-afd5a051e8cd"},"source":["%cd $citation_tagging_dir\r\n","from BiLSTM_CRF import BiLSTM_CRF\r\n","import torch\r\n","torch.manual_seed(1)\r\n","\r\n","with open(citation_tagging_dir + 'bilstm_word_to_ix.pickle', 'rb') as dbfile:\r\n","  word_to_ix = pickle.load(dbfile)\r\n","bilstm_tagger = torch.load(citation_tagging_dir + 'BiLSTMfullmodel.pth')\r\n","bilstm_tagger.eval()\r\n","\r\n","ix_to_tag = {0: 'A', 1: 'D', 2: 'T', 3: 'O', 4: '<START>', 5: '<STOP>'}\r\n","def bilstm_get_tags(raw_text):\r\n","  \"\"\"Uses the BiLSTM tagger to output predicted tags.\"\"\"\r\n","  with torch.no_grad():\r\n","    prepared_cit = prepare_sequence(raw_text.split(), word_to_ix)\r\n","    predicted_tags = bilstm_tagger(prepared_cit)[1]\r\n","    return ' '.join([ix_to_tag[pred] for pred in predicted_tags])\r\n","\r\n","def prepare_sequence(seq, to_ix):\r\n","  \"\"\"Replaces words with numbers in a sequence based on the mapping TO_IX.\"\"\"\r\n","  # This is a potential bug: I used the get method with a default to keep the\r\n","  # program from erroring out, but what if a large number of words are not in\r\n","  # the dictionary?\r\n","  idxs = [to_ix.get(w, to_ix['the']) for w in seq]\r\n","  return torch.tensor(idxs, dtype=torch.long)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/.shortcut-targets-by-id/1W2EROe2FItlaK99U-WY_qaBOc2UD_LI0/AWCA/Colab_notebooks/CitationTagging\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kmTT0dw1n1-B"},"source":["class BiLSTMTaggerWrapper(Testable):\r\n","  def predict(self, test):\r\n","    preproc = test.str.replace('[,.;:\\'\\\"]', '')\r\n","    preproc = preproc.str.replace('\\(|[a-z]\\)|\\)', '')\r\n","    preproc = preproc.str.replace('\\d\\d\\d\\d(–|-)\\d\\d\\d\\d', 'dateRange')\r\n","    preproc = preproc.str.replace('\\d\\d\\d\\d', 'fourDigitNum')\r\n","    preproc = preproc.str.replace('[xiv]+(–|-)[xiv]+', 'numerals')\r\n","    preproc = preproc.str.replace('\\d+(–|-)\\d+', 'pageRange')\r\n","    preproc = preproc.str.replace('\\d+', 'otherDigits')\r\n","    preproc = preproc.str.lower()\r\n","    tags = [bilstm_get_tags(text) for text in preproc]\r\n","    ret = expand_raw_and_tags(pd.DataFrame({\r\n","        'raw_text': test, 'tags': tags\r\n","    }))\r\n","    years = np.zeros(len(ret.year))\r\n","    for i, year in enumerate(ret.year):\r\n","      match = re.search('\\d\\d\\d\\d', year)\r\n","      if match:\r\n","        years[i] = int(match.group(0))\r\n","      else:\r\n","        years[i] = np.NaN\r\n","    ret.drop('year', axis=1)\r\n","    ret['year'] = pd.Series(years, index=ret.index)\r\n","    return ret"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-hB0bgXv9OTB"},"source":["Here, I verify that the results in Accuracy.ipynb are being replicated correctly for the **Zotero datasets**. This is necessary to gain confidence that there are no bugs in this notebook."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OMLiSVA-BaOt","outputId":"ffab8515-71d8-489c-c213-27482f41b165"},"source":["btw = BiLSTMTaggerWrapper()\r\n","print('Results for Running the BiLSTM Tagger on the Zotero Training Set:')\r\n","TestRunner(btw, zotero_preproc_train).report(1000)\r\n","print('\\nResults for Running the BiLSTM Tagger on the Zotero Test Set:')\r\n","TestRunner(btw, zotero_preproc_test).report(1000)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Results for Running the BiLSTM Tagger on the Zotero Training Set:\n","Time to get predictions: 38.2610 seconds.\n","Proportion of tokens correctly tagged: 0.8821. 95% CI: (0.8787, 0.8851)\n","Proportion of citations for which every word corresponding\n","to a contributor's name is correctly tagged as such: 0.9045. 95% CI: (0.8985, 0.9099)\n","Proportion of citations for which every word corresponding\n","to a part of a title is correctly tagged as such: 0.8159. 95% CI: (0.8082, 0.8234)\n","Proportion of citations for which every word corresponding\n","to the year of publication is correctly tagged as such: 0.8285. 95% CI: (0.8208, 0.8353)\n","Proportion of citations that have a non-null value in the\n","tags column for which the value in that column is predicted\n","correctly: 0.3846. 95% CI: (0.3749, 0.3931)\n","Proportion of citations that have a non-null value in the\n","contributors column for which the value in that column is predicted\n","correctly: 0.6884. 95% CI: (0.6800, 0.6982)\n","Proportion of citations that have a non-null value in the\n","title column for which the value in that column is predicted\n","correctly: 0.4977. 95% CI: (0.4880, 0.5067)\n","Proportion of citations that have a non-null value in the\n","year column for which the value in that column is predicted\n","correctly: 0.0000. 95% CI: (0.0000, 0.0000)\n","Proportion of titles predicted approximately correctly: 0.4977. 95% CI: (0.4882, 0.5078)\n","\n","Results for Running the BiLSTM Tagger on the Zotero Test Set:\n","Time to get predictions: 216.3109 seconds.\n","Proportion of tokens correctly tagged: 0.8768. 95% CI: (0.8755, 0.8781)\n","Proportion of citations for which every word corresponding\n","to a contributor's name is correctly tagged as such: 0.8899. 95% CI: (0.8873, 0.8927)\n","Proportion of citations for which every word corresponding\n","to a part of a title is correctly tagged as such: 0.8014. 95% CI: (0.7982, 0.8045)\n","Proportion of citations for which every word corresponding\n","to the year of publication is correctly tagged as such: 0.8302. 95% CI: (0.8270, 0.8332)\n","Proportion of citations that have a non-null value in the\n","tags column for which the value in that column is predicted\n","correctly: 0.3652. 95% CI: (0.3612, 0.3693)\n","Proportion of citations that have a non-null value in the\n","contributors column for which the value in that column is predicted\n","correctly: 0.6685. 95% CI: (0.6646, 0.6726)\n","Proportion of citations that have a non-null value in the\n","title column for which the value in that column is predicted\n","correctly: 0.4781. 95% CI: (0.4741, 0.4825)\n","Proportion of citations that have a non-null value in the\n","year column for which the value in that column is predicted\n","correctly: 0.0000. 95% CI: (0.0000, 0.0000)\n","Proportion of titles predicted approximately correctly: 0.4781. 95% CI: (0.4741, 0.4823)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pHHSMjh8NZeW"},"source":["I observe that all metrics that appear in [Accuracy.ipynb](https://colab.research.google.com/drive/1D7i5pLgqEsrLG3PdwiZKcSMOfafWvztl) are within $16\\times10^{-4}$ of what is reported here. Therefore I am happy, and I claim that Jason Webb's results are being correctly reproduced.\r\n","\r\n","I now run the tagger on the **OCC dataset**:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rHj1bfmD88Dn","outputId":"c92249ef-1867-4644-b2c6-5ca2f9fd5990"},"source":["TestRunner(btw, occ).report(1000)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Time to get predictions: 30.6852 seconds.\n","Proportion of tokens correctly tagged: 0.5445. 95% CI: (0.5389, 0.5498)\n","Proportion of citations for which every word corresponding\n","to a contributor's name is correctly tagged as such: 0.0778. 95% CI: (0.0702, 0.0849)\n","Proportion of citations for which every word corresponding\n","to a part of a title is correctly tagged as such: 0.8946. 95% CI: (0.8867, 0.9034)\n","Proportion of citations for which every word corresponding\n","to the year of publication is correctly tagged as such: 0.4798. 95% CI: (0.4666, 0.4940)\n","Proportion of citations that have a non-null value in the\n","tags column for which the value in that column is predicted\n","correctly: 0.0000. 95% CI: (0.0000, 0.0000)\n","Proportion of citations that have a non-null value in the\n","contributors column for which the value in that column is predicted\n","correctly: 0.0025. 95% CI: (0.0012, 0.0042)\n","Proportion of citations that have a non-null value in the\n","title column for which the value in that column is predicted\n","correctly: 0.0000. 95% CI: (0.0000, 0.0000)\n","Proportion of citations that have a non-null value in the\n","year column for which the value in that column is predicted\n","correctly: 0.8457. 95% CI: (0.8348, 0.8557)\n","Proportion of titles predicted approximately correctly: 0.0017. 95% CI: (0.0006, 0.0029)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qW7_Wyz3H_Cl"},"source":["These results are rather different from the results from the Zotero dataset, but what does this mean? Does it mean that the tags that I added to the OCC dataset are wrong, or does it instead mean that the tags of the OCC dataset are simply different from the tags in the Zotero dataset? If it is the latter, then that would imply that tagging involves subjectivity, in which case it might not be the ideal means for evaluating a model.\r\n","\r\n","If it is the former, then, well, I ought revise my (rather messy) tagging function at the bottom of [this notebook](https://github.com/petervdonovan/CitationParser/blob/master/datasets/ccc_dataset.ipynb)."]}]}