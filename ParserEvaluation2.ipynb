{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ParserEvaluation2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPSdwVrkG36T0rfQgOQCI5i"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"V0lYhtjCpq9X"},"source":["# Parser Evaluation\r\n","\r\n","There is an [earlier version](https://colab.research.google.com/github/petervdonovan/CitationParser/blob/master/ParserEvaluation.ipynb) of this notebook, and much of the work contained in this notebook is different from but parallel to this earlier version. There are a few reasons why it was necessary to rethink this prior work:\r\n","1. Tags will not be a component of the testing or training dataset.\r\n","    * This does not mean that tags will not continue to be important. However, I have decided that the our objective -- use of citations to create edges in a social network -- does not inherently involve tagging.\r\n","1. In lieu of tags, metadata is being included in a format that is as independent from its origin as possible.\r\n","    * Names in particular do require this. In bibliographic entries, the formatting of names varies widely by style guide. If names are to be considered as semantic information instead of as raw text, they should be presented in a data structure that makes the different parts of a name explicit.\r\n","1. Metrics used to evaluate models will be carefully chosen to make them as interpretable as possible for individuals who do not know any low-level details of how our models operate.\r\n","    * Changes in this direction reduce the amount of text that is required alongside any reported metrics.\r\n","\r\n","There may be several parts of this notebook where it is tempting to cry out, \"I have seen all of this before, in another notebook! This is not DRY!\" I insist on justifying myself by noting that in writing this notebook I have slightly different purposes in mind, and that I do not wish to be constrained by links or dependencies with an implementation that I may not wish to keep."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iqBT1GGgy6X1","executionInfo":{"status":"ok","timestamp":1614318217049,"user_tz":480,"elapsed":2967,"user":{"displayName":"Peter Donovan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmevrWa6lY-MoKL6lRCYdlS09IgSTYxUPNKCYZ=s64","userId":"01568505250649921698"}},"outputId":"41174cb3-9574-424d-8203-0e9bd73248cb"},"source":["!pip install pickle5\r\n","import pandas as pd\r\n","import numpy as np\r\n","import pickle5 as pickle\r\n","import random\r\n","import time\r\n","import matplotlib.pyplot as plt\r\n","from gensim.utils import simple_preprocess\r\n","from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: pickle5 in /usr/local/lib/python3.7/dist-packages (0.0.11)\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"R3I_B8gFwTD8"},"source":["## Datasets\r\n","\r\n","### Note on dataset selection\r\n","\r\n","For the moment, I will be loading the OpenCitations dataset for initial tests. Much of the motivation behind our use of this dataset is replicability, although it does have a replicability issue of its own: Because the OpenCitations dataset is too large to support a SPARQL query that asks for the DOI of _every_ cited work for which we have a raw-text citation, I had to use a LIMIT clause. There are no promises about whether the sample I got was representative of the corpus, so this is a little unfortunate.\r\n","\r\n","Another challenge is that the Zotero dataset does not have raw text citations that are reliably matched with the corresponding metadata. A _mostly_ successful attempt to tackle this issue is included in [this notebook](https://colab.research.google.com/drive/1OEFWVWgEzCiPA35Ma20svEY5OhPCuPwI), but it is a beast, and the output has imperfections of a severity that is difficult to quantify definitively.\r\n","\r\n","In any case, the OpenCitations dataset will be used for development. Long-term goals might be:\r\n","* To include a more representative (or complete) sample of the OpenCitations corpus, for replicability, and\r\n","* To include the Zotero dataset.\r\n","\r\n","### Fields in the Dataset\r\n","\r\n","The dataset has the following fields for the models to use for prediction:\r\n","* **raw_text**: The raw text citation, represented as a string.\r\n","\r\n","The dataset has the following fields for the models to predict:\r\n","* **author**: The name(s) of the authors of the work, represented as `Name` objects.\r\n","* **year**: The publication year of the work, represented as an integer.\r\n","* **title**: The title of the work, represented as a string.\r\n","\r\n","These three fields are the ones that are likely candidates to be used to construct edges; the others are much more questionable. Each of them is commonly found -- for example, 98.8% of records in the Zotero database have a publication year, 99.6% have an author, and 99.7% have a title.\r\n","\r\n","Every row used in this dataset will have all three fields, i.e., the datasets will look like someone called `dropna` on them to eliminate null values in those columns. I acknowledge that this means that what is left may not be representative of the raw-text citations that may be out there in the wild; however, as mentioned in the above paragraph, those three fields are fairly commonplace.\r\n","\r\n","It is worth noting, however, that other fields are included in case we decide to incorporate them into our analysis later. They are less common and less reliable, so I would only include them as a very late micro-optimization to our model, if at all.\r\n","* pages: The pages in which the work appeared in the container (book, anthology, journal issue, etc.) in which it was published, represented as a string containing digits, a hyphen, and then more digits (or just digits, if the work appears on only one page)\r\n","* volume: The volume in which the work appears, represented as a string.\r\n","* source_title: The name of the journal or other entity responsible for the publication of the work.\r\n","* issue: The issue in which the work appeared.\r\n","\r\n","Matching surnames, matching titles, and matching years should be sufficient grounds for declaring a match. `simple_preprocess` from Gensim.utils with deacc=True should be enough to get reliable matches, if the fields can just be extracted correctly.\r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"-xBDR6gfm1go"},"source":["# The 46K-row OpenCitations dataset will be loaded here."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yjN3O7HrxZ80"},"source":["## TestRunner\r\n","\r\n","As before, I define a TestRunner class. For now at least, I am choosing to simply stick with percent accuracy in predicting year, author, and title, because the frequency with which those attributes give successful matches with the reported metadata should be a good predictor of the frequency with which they give successful matches between different texts in a corpus.\r\n","\r\n","Logic for k-fold CVs or LOOCVs is not managed by this class. This class is strictly for computing and reporting metrics on a given dataset."]},{"cell_type":"code","metadata":{"id":"Az6xNkIrpgfj"},"source":["class TestRunner:\r\n","  \"\"\"Encapsulates logic for calculating model performance statistics.\"\"\"\r\n","  def __init__(self, model, test_set):\r\n","    \"\"\"Initializes the TestRunner with a trained MODEL that is Testable and a\r\n","    TEST_SET, which is a DataFrame that has the columns specified in the above\r\n","    section, \"Loading Datasets.\"\r\n","    \"\"\"\r\n","    self.model = model\r\n","    self.test_set = test_set\r\n","    # self.predictions will be a DataFrame of the same format as TEST_SET.\r\n","    # However, it is set to None here because it will be computed lazily.\r\n","    self.predictions = None\r\n","  def get_predictions(self):\r\n","    \"\"\"Returns a DataFrame with the same columns as the DataFrame on which the\r\n","    model was trained, as described in the \"Datasets\" section of this notebook.\r\n","    \"\"\"\r\n","    if self.predictions is None:\r\n","      self.predictions = self.model.predict(self.test_set.raw_text)\r\n","    return self.predictions\r\n","  def year_accuracy(self):\r\n","    \"\"\"Returns the proportion of publication years that the model predicts\r\n","    accurately.\r\n","    \"\"\"\r\n","    return np.mean(self.test_set.year == self.get_predictions().year)\r\n","  def surname_accuracy(self):\r\n","    \"\"\"Returns the proportion of citations from which the set of all authors'\r\n","    surnames is predicted with perfect accuracy (within the simplifications of\r\n","    de-accenting and capitalization/punctuation removal).\r\n","    \"\"\"\r\n","    return np.mean([\r\n","        (\r\n","            set(simple_preprocess(name.surname) for name in true_names)\r\n","            == set(simple_preprocess(name.surname) for name in predicted_names)\r\n","        ) for true_names, predicted_names\r\n","        in zip(self.test_set.author, self.get_predictions().author)\r\n","    ])\r\n","  def primary_author_surname_accuracy(self):\r\n","    \"\"\"Returns the proportion of citations from which the primary author's\r\n","    surname is predicted correctly. The first author who is listed in the\r\n","    dataset will be interpreted as the primary author.\r\n","    \"\"\"\r\n","    return np.mean([\r\n","        true_names[0].surname == predicted_names[0].surname\r\n","        for true_names, predicted_names\r\n","        in zip(self.test_set.author, self.get_predictions().author)\r\n","    ])\r\n","  def title_accuracy(self):\r\n","    \"\"\"Returns the proportion of citations from which the title of the cited work\r\n","    is predicted with perfect accuracy (within the simplifications of\r\n","    de-accenting and capitalization/punctuation removal).\r\n","    \"\"\"\r\n","    return np.mean(\r\n","        simple_preprocess(true_title) == simple_preprocess(predicted_title)\r\n","        for true_title, predicted_title\r\n","        in zip(self.test_set.title, self.get_predictions().title)\r\n","    )\r\n","  def quick_report(self):\r\n","    \"\"\"Prints a report of model performance without confidence intervals.\r\n","    Intended for development (debugging, preliminary results, etc.) and not for\r\n","    final reporting.\r\n","    \"\"\"\r\n","    t0 = time.time()\r\n","    get_predictions()\r\n","    elapsed_time = time.time() - t0\r\n","    print('Generated labels for {0:,} records in {:.4f} seconds ({:.4f} seconds'\r\n","          ' per record)'.format(\r\n","              len(self.test_set.index),\r\n","              elapsed_time,\r\n","              elapsed_time / len(self.test_set.index)))\r\n","    print('Surname accuracy (complete set): {:.4f}'.format(\r\n","        self.surname_accuracy()))\r\n","    print('Year accuracy: {:.4f}'.format(self.year_accuracy()))\r\n","    print('Title accuracy (with preproc): {:.4f}').format(self.title_accuracy())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eqx8G-vI7wii"},"source":["This demonstrates the power of `simple_preprocess`, a simple, widely used utility whose function should be reasonably easy to explain to other people."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2pQPhdt32_rR","executionInfo":{"status":"ok","timestamp":1614319554470,"user_tz":480,"elapsed":304,"user":{"displayName":"Peter Donovan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmevrWa6lY-MoKL6lRCYdlS09IgSTYxUPNKCYZ=s64","userId":"01568505250649921698"}},"outputId":"4cfcbfbc-12aa-4ca0-d105-dc91efbe0f81"},"source":["simple_preprocess('The cát, in the hát.', deacc=True) == simple_preprocess('the cAt  iN tHe haT!!!?', deacc=True)"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"bPviJEg5DY8P"},"source":["class CrossValidator:\r\n","  \"\"\"Reports cross validation results for a given model and dataset.\"\"\"\r\n","  # This should have a has-a relationship with TestRunner.\r\n","  # You know, it's possible sklearn has this. I will take a look."],"execution_count":null,"outputs":[]}]}