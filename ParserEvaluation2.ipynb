{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ParserEvaluation2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPSQQxLc+0znzIiNz67/ZXe"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"V0lYhtjCpq9X"},"source":["# Parser Evaluation\n","\n","There is an [earlier version](https://colab.research.google.com/github/petervdonovan/CitationParser/blob/master/ParserEvaluation.ipynb) of this notebook, and much of the work contained in this notebook is different from but parallel to this earlier version. There are a few reasons why it was necessary to rethink this prior work:\n","\n","1. Tags will not be a component of the testing or training dataset.\n","\n","    * This does not mean that tags will not continue to be important. However, I have decided that the our objective -- use of citations to create edges in a social network -- does not inherently involve tagging.\n","\n","1. In lieu of tags, metadata is being included in a format that is as independent from its origin as possible.\n","\n","    * Names in particular do require this. In bibliographic entries, the formatting of names varies widely by style guide. If names are to be considered as semantic information instead of as raw text, they should be presented in a data structure that makes the different parts of a name explicit.\n","\n","1. Metrics used to evaluate models will be carefully chosen to make them as interpretable as possible for individuals who do not know any low-level details of how our models operate.\n","\n","    * Changes in this direction reduce the amount of text that is required alongside any reported metrics.\n","\n","There may be several parts of this notebook where you may cry out, \"I have seen all of this before, in another notebook! This is not DRY!\" I insist on justifying myself by noting that in writing this notebook I have a slightly different purpose in mind, and that I do not wish to be constrained by any links or dependencies on an implementation that I may not wish to keep."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iqBT1GGgy6X1","executionInfo":{"status":"ok","timestamp":1614578647271,"user_tz":480,"elapsed":3537,"user":{"displayName":"Peter Donovan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmevrWa6lY-MoKL6lRCYdlS09IgSTYxUPNKCYZ=s64","userId":"01568505250649921698"}},"outputId":"3bff0e34-a48f-4825-ee89-730b7cdc969f"},"source":["!pip install pickle5\n","import pandas as pd\n","import numpy as np\n","import pickle5 as pickle\n","import random\n","import time\n","import itertools\n","import os\n","import matplotlib.pyplot as plt\n","from gensim.utils import simple_preprocess\n","\n","from People.NameList import NameList\n","#from google.colab import drive\n","#drive.mount('/content/drive')\n","#%cd /content/drive/MyDrive/AWCA/Colab_notebooks/CitationTagging/Sp21/CitationParser/"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pickle5 in ./env/lib/python3.8/site-packages (0.0.11)\n"]}]},{"cell_type":"markdown","metadata":{"id":"R3I_B8gFwTD8"},"source":["## Datasets\n","\n","### Note on dataset selection\n","\n","For the moment, I will be loading the OpenCitations dataset for initial tests. There are a couple of serious limitations to this dataset which we should be aware of:\n","* It has a replicability issue: Because the OpenCitations dataset is too large to support a SPARQL query that asks for the DOI of _every_ cited work for which we have a raw-text citation, I had to use a LIMIT clause. There are no promises about whether the sample I got was representative of the corpus, so this is a little unfortunate.\n","* It is probably dissimilar to our dataset in important ways. Specifically, many of the bibliographic entities documented in the particular portion of the dataset that I have downloaded are journal articles on biology and medicine -- a fact that may be reflected in the style guides that are used.\n","\n","For these reasons, OpenCitations is an additional tool, not a replacement for our other datasets. I am using it for the moment so that I can focus on developing a parser using a clean dataset, without having to worry about whether my results are valid or whether the dataset generation process has bugs.\n","\n","Another challenge is that the Zotero dataset does not have raw text citations that are reliably matched with the corresponding metadata. A _mostly_ successful attempt to tackle this issue is included in [this notebook](https://colab.research.google.com/drive/1OEFWVWgEzCiPA35Ma20svEY5OhPCuPwI), but it is a beast, and the output has imperfections of a severity that is difficult to quantify definitively.\n","\n","In any case, the OpenCitations dataset will be used for development. Long-term goals might be:\n","* To include a more representative (or complete) sample of the OpenCitations corpus, for replicability, and\n","* To include the Zotero dataset.\n","\n","### Fields in the Dataset\n","\n","The dataset has the following fields for the models to use for prediction:\n","* **raw**: The raw text citation, represented as a string.\n","\n","The dataset has the following fields for the models to predict:\n","* **author**: The name(s) of the authors of the work, represented as `NameList` objects.\n","* **year**: The publication year of the work, represented as an integer.\n","* **title**: The title of the work, represented as a string.\n","\n","These three fields are the ones that are likely candidates to be used to construct edges; the others are much more questionable. Each of them is commonly found -- for example, 98.8% of records in the Zotero database have a publication year, 99.6% have an author, and 99.7% have a title.\n","\n","Every row used in this dataset will have all three fields, i.e., the datasets will look like someone called `dropna` on them to eliminate null values in those columns. I acknowledge that this means that what is left may not be representative of the raw-text citations that may be out there in the wild; however, as mentioned in the above paragraph, those three fields are fairly commonplace.\n","\n","It is worth noting, however, that other fields are available in case we decide to incorporate them into our analysis later. They are less common and less reliable, so I would only include them as a very late micro-optimization to our model, if at all.\n","* pages: The pages in which the work appeared in the container (book, anthology, journal issue, etc.) in which it was published, represented as a string containing digits, a hyphen, and then more digits (or just digits, if the work appears on only one page)\n","* volume: The volume in which the work appears, represented as a string.\n","* source_title: The name of the journal or other entity responsible for the publication of the work.\n","* issue: The issue in which the work appeared.\n","\n","Matching surnames, matching titles, and matching years should be sufficient grounds for declaring a match. `simple_preprocess` from Gensim.utils with deacc=True should be enough to get reliable matches, if the fields can just be extracted correctly.\n","\n"]},{"cell_type":"code","metadata":{"id":"-xBDR6gfm1go","colab":{"base_uri":"https://localhost:8080/","height":174},"executionInfo":{"status":"ok","timestamp":1614579656812,"user_tz":480,"elapsed":540,"user":{"displayName":"Peter Donovan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmevrWa6lY-MoKL6lRCYdlS09IgSTYxUPNKCYZ=s64","userId":"01568505250649921698"}},"outputId":"5e246476-2ef1-4cf2-f90f-3c829fd0f92e"},"source":["with open('datasets/occ_45K.pickle', 'rb') as dbfile:\n","  occ_45K = pickle.load(dbfile)\n","occ = pd.DataFrame()\n","occ['raw']    = occ_45K.raw\n","occ['author'] = occ_45K.author\n","occ['year']   = occ_45K.year\n","occ['title']  = occ_45K.title\n","print('I wished to get metadata for {} DOIs downloaded from the OpenCitations\\n'\n","      'SPARQL endpoint, '.format(\n","    len(occ.index)\n","), end='')\n","occ = occ.dropna()\n","print('but only {} complete rows of metadata were received.'.format(\n","    len(occ.index)\n","))\n","occ.sample(2)"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["I wished to get metadata for 45756 DOIs downloaded from the OpenCitations\nSPARQL endpoint, but only 45280 complete rows of metadata were received.\n"]},{"output_type":"execute_result","data":{"text/plain":["                                                     raw  \\\n","6629   Blennow, G, McNeil, TF. Neurological deviation...   \n","33955  Dandawate, P, Padhye, S, Ahmad, A, Sarkar, FH....   \n","\n","                                                  author  year  \\\n","6629                          Blennow, G.; Mcneil, T. F.  1991   \n","33955  Dandawate, Prasad; Padhye, Subhash; Ahmad, Aam...  2012   \n","\n","                                                   title  \n","6629   Neurological Deviations In Newborns At Psychia...  \n","33955  Novel Strategies Targeting Cancer Stem Cells T...  "],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>raw</th>\n      <th>author</th>\n      <th>year</th>\n      <th>title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6629</th>\n      <td>Blennow, G, McNeil, TF. Neurological deviation...</td>\n      <td>Blennow, G.; Mcneil, T. F.</td>\n      <td>1991</td>\n      <td>Neurological Deviations In Newborns At Psychia...</td>\n    </tr>\n    <tr>\n      <th>33955</th>\n      <td>Dandawate, P, Padhye, S, Ahmad, A, Sarkar, FH....</td>\n      <td>Dandawate, Prasad; Padhye, Subhash; Ahmad, Aam...</td>\n      <td>2012</td>\n      <td>Novel Strategies Targeting Cancer Stem Cells T...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":2}]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["...but even the rows of metadata that did not explicitly have\nnull values such as None or np.NaN sometimes had unusable data.\n1192 rows had no author names or author names that were formatted\ninconsistently or in a way that was difficult to interpret, 167\nrows had no years provided, and 137 rows had no titles provided.\nIn total, 1448 rows had to be excluded for these reasons, \nleaving 43832 rows of truly complete metadata.\n"]}],"source":["bad_author_idx = [\n","    idx for idx in occ.index\n","    if (not occ.author[idx]) or any(\n","        len(name.split(',')) != 2\n","        for name in occ.author[idx].split(';')\n","    )\n","]\n","bad_title_idx = [idx for idx in occ.index if not occ.title[idx]]\n","bad_year_idx = [idx for idx in occ.index if not occ.year[idx]]\n","\n","occ_drop_idx = list(\n","    set(bad_author_idx) | set(bad_title_idx) | set(bad_year_idx))\n","occ = occ.drop(occ_drop_idx, axis=0)\n","\n","print('...but even the rows of metadata that did not explicitly have\\n'\n","      'null values such as None or np.NaN sometimes had unusable data.\\n'\n","      '{} rows had no author names or author names that were formatted\\n'\n","      'inconsistently or in a way that was difficult to interpret, {}\\n'\n","      'rows had no years provided, and {} rows had no titles provided.\\n'\n","      'In total, {} rows had to be excluded for these reasons, \\n'\n","      'leaving {} rows of truly complete metadata.'.format(\n","          len(bad_author_idx),\n","          len(bad_title_idx),\n","          len(bad_year_idx),\n","          len(occ_drop_idx),\n","          len(occ.index)\n","      ))"]},{"cell_type":"markdown","metadata":{"id":"rJybdCVmbCbS"},"source":["In short, 5000 DOIs were requested, 45756 DOIs were received. Then 45756 rows of complete metadata were requested, and of them, 43832 complete rows of metadata were received. This means that after the crucial bottleneck where I was only able to request an arbitrarily (and not necessarily randomly) selected sample of 50000 DOIs, I was able to get most of the data I wanted."]},{"cell_type":"markdown","metadata":{"id":"KojOSlLjcpBI"},"source":["Let us get a sense for how clean and complete the dataset is. Do the raws really contain sufficient information to get the data we want? How can this inform the expectations and concerns we might have when developing a parser?\r\n","\r\n","The following cells may be run several times to see many different rows in the dataset."]},{"cell_type":"markdown","metadata":{"id":"K9_fqZCIebLI"},"source":["Already, it is starting to seem like most author surnames seem to be available. However, even a human processing the dataset by hand might find it difficult or impossible to determine given names. Sometimes, it is even difficult to determine whose given names or initials are whose."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":195},"id":"Kx6S9Y9ic_Do","executionInfo":{"status":"ok","timestamp":1614580127293,"user_tz":480,"elapsed":295,"user":{"displayName":"Peter Donovan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmevrWa6lY-MoKL6lRCYdlS09IgSTYxUPNKCYZ=s64","userId":"01568505250649921698"}},"outputId":"52f3e857-5b7a-46f6-b81d-180ac4be7252"},"source":["occ.sample(5).drop(['year', 'title'], axis=1)"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                     raw  \\\n","41538  Kounoue, E, Izumi, K, Ogawa, S, Kondo, S, Kats...   \n","37136  an, N.H, Arunmozhiarasi, A, Ponnudurai, G. A c...   \n","6100   Manikandan S. Are we moving towards a new defi...   \n","23099  van den Brand, JM, Haagmans, BL, van Riel, D, ...   \n","9335   Ertas, G, Gulcur, HO, Osman, O, Ucan, ON, Tuna...   \n","\n","                                                  author  \n","41538  Kounoue, Etsushi; Izumi, Ken-Ichi; Ogawa, Shui...  \n","37136  Tan, Nget-Hong; Arunmozhiarasi, Armugam; Ponnu...  \n","6100                                       Manikandan, S  \n","23099  Van Den Brand, J.M.A.; Haagmans, B.L.; Van Rie...  \n","9335   Ertaş, Gökhan; Gülçür, H.Özcan; Osman, Onur; U...  "],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>raw</th>\n      <th>author</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>41538</th>\n      <td>Kounoue, E, Izumi, K, Ogawa, S, Kondo, S, Kats...</td>\n      <td>Kounoue, Etsushi; Izumi, Ken-Ichi; Ogawa, Shui...</td>\n    </tr>\n    <tr>\n      <th>37136</th>\n      <td>an, N.H, Arunmozhiarasi, A, Ponnudurai, G. A c...</td>\n      <td>Tan, Nget-Hong; Arunmozhiarasi, Armugam; Ponnu...</td>\n    </tr>\n    <tr>\n      <th>6100</th>\n      <td>Manikandan S. Are we moving towards a new defi...</td>\n      <td>Manikandan, S</td>\n    </tr>\n    <tr>\n      <th>23099</th>\n      <td>van den Brand, JM, Haagmans, BL, van Riel, D, ...</td>\n      <td>Van Den Brand, J.M.A.; Haagmans, B.L.; Van Rie...</td>\n    </tr>\n    <tr>\n      <th>9335</th>\n      <td>Ertas, G, Gulcur, HO, Osman, O, Ucan, ON, Tuna...</td>\n      <td>Ertaş, Gökhan; Gülçür, H.Özcan; Osman, Onur; U...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"dunnq7gle2ab"},"source":["Sometimes, titles do not end with a period or closing quotation mark; instead, they terminate with a comma, potentially making it difficult even for a human to know when the title ends and when other article data begins.\r\n","\r\n","At least a few different style guides do seem to be represented, each with different schemes for where to place the year, how to format the title, and so forth."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XuiDpHG5dh1g","executionInfo":{"status":"ok","timestamp":1614580666728,"user_tz":480,"elapsed":258,"user":{"displayName":"Peter Donovan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmevrWa6lY-MoKL6lRCYdlS09IgSTYxUPNKCYZ=s64","userId":"01568505250649921698"}},"outputId":"faae10b0-d05c-4d8c-8b5e-1e708c2582bd"},"source":["list(occ.sample(5).raw)"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Zbinden C. Leader neurons in leaky integrate and fire neural network simulations. J Comput Neurosci 31, 285–304 (2011). PMID: 21234795',\n"," 'Davis GE (1939) Ornithodoros parkeri: Distribution and host data; spontaneous infection with relapsing fever spirochetes. Public Health Rep 54: 1345–1349.',\n"," 'Talbot, H.M, Summons, R.E, Jahnke, L.L, Cockell, C.S, Rohmer, M, and Farrimond, P. (2008) Cyanobacterial bacteriohopanepolyol signatures from cultures and natural environmental settings. Org Geochem 39: 232–263. doi: 10.1016/j.orggeochem.2007.08.006.',\n"," 'Kamal M. A. et al. Kinetics of human serum butyrylcholinesterase inhibition by a novel experimental Alzheimer therapeutic, dihydrobenzodioxepine cymserine. Neurochemical research 33, 745–753, DOI: 10.1007/s11064-007-9490-y (2008). PMID: 17985237',\n"," 'Schuler JR, Bockisch CJ, Straumann D, Tarnutzer AA. Precision and accuracy of the subjective haptic vertical in the roll plane. BMC Neurosci. BioMed Central Ltd; 2010; 11: 83 doi: 10.1186/1471-2202-11-83 PMID: 20630097']"]},"metadata":{},"execution_count":5}]},{"source":["## Name Raw Text -> NameList Objects\n","\n","The purpose of this step is simply to better organize the data according to its semantics. No interesting analysis is going on here.\n","\n","Although the DataFrame will look the same after this step is applied because of how the __str__ method is implemented for NameLists, the objects that are actually being stored in the DataFrame will have some methods available to make explicit the distinctions between surnames and given names, and between different authors' names."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                 raw  \\\n","1  Knechtle, B, Knechtle, P, Schulze, I, Kohler, ...   \n","2  Sousa, M, Fernandes, MJ, Moreira, P, Teixeira,...   \n","\n","                                              author  year  \\\n","1                          Knechtle, B.; Schulze, I.  2008   \n","2  Sousa, Mónica; Fernandes, Maria João; Moreira,...  2013   \n","\n","                                               title  \n","1  Ernährungsverhalten Bei Ultraläufern - Deutsch...  \n","2  Nutritional Supplements Usage By Portuguese At...  "],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>raw</th>\n      <th>author</th>\n      <th>year</th>\n      <th>title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>Knechtle, B, Knechtle, P, Schulze, I, Kohler, ...</td>\n      <td>Knechtle, B.; Schulze, I.</td>\n      <td>2008</td>\n      <td>Ernährungsverhalten Bei Ultraläufern - Deutsch...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Sousa, M, Fernandes, MJ, Moreira, P, Teixeira,...</td>\n      <td>Sousa, Mónica; Fernandes, Maria João; Moreira,...</td>\n      <td>2013</td>\n      <td>Nutritional Supplements Usage By Portuguese At...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":6}],"source":["occ.author = occ.apply(lambda row: NameList.delimited(row.author), axis=1)\n","occ.head(2)"]},{"source":["## Train-Test Split\n","\n","I have decided (somewhat arbitrarily) on the following train-test split:\n","\n","* 10,000 items will be set aside for testing\n","\n","* 10,000 items will be set aside for validation\n","\n","* The remaining (~24,000) items will be set aside for training"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# The random state is the hour and minute when I wrote this code.\n","occ_test = occ.sample(n=10000, replace=False, random_state=11)\n","occ_remaining = occ.drop(occ_test.index, axis=0)\n","occ_validation = occ_remaining.sample(n=10000, replace=False, random_state=41)\n","occ_train = occ_remaining.drop(occ_validation.index, axis=0)\n","# Let's be extra sure this operation was done correctly!\n","for a, b in itertools.combinations(\n","        [occ_test.index, occ_validation.index, occ_train.index],\n","        2):\n","    assert not any(element in b for element in a)"]},{"source":["The code used to save the datasets is included here for transparency, but the conditional blocks will prevent it from being run again."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["assert os.path.exists('./datasets/')\n","test_path = './datasets/occ_45K_test.pickle'\n","validation_path = './datasets/occ_45K_validation.pickle'\n","train_path = './datasets/occ_45K_train.pickle'\n","if not os.path.exists(test_path):\n","    with open(test_path, 'ab') as dbfile:\n","        pickle.dump(occ_test, dbfile, protocol=pickle.HIGHEST_PROTOCOL)\n","if not os.path.exists(validation_path):\n","    with open(validation_path, 'ab') as dbfile:\n","        pickle.dump(occ_validation, dbfile, protocol=pickle.HIGHEST_PROTOCOL)\n","if not os.path.exists(train_path):\n","    with open(train_path, 'ab') as dbfile:\n","        pickle.dump(occ_train, dbfile, protocol=pickle.HIGHEST_PROTOCOL)"]},{"cell_type":"markdown","metadata":{"id":"yjN3O7HrxZ80"},"source":["## TestRunner\r\n","\r\n","As before, I define a TestRunner class. For now at least, I am choosing to simply stick with percent accuracy in predicting year, author, and title as my performance metrics, because the frequency with which those attributes give successful matches with the reported metadata should be a good predictor of the frequency with which they give successful matches between different texts in a corpus.\r\n","\r\n","Logic for k-fold CVs or LOOCVs is not managed by this class. This class is strictly for computing and reporting metrics on a given dataset."]},{"cell_type":"code","metadata":{"id":"Az6xNkIrpgfj"},"source":["class TestRunner:\r\n","  \"\"\"Encapsulates logic for calculating model performance statistics.\"\"\"\r\n","  def __init__(self, model, test_set):\r\n","    \"\"\"Initializes the TestRunner with a trained MODEL that is Testable and a\r\n","    TEST_SET, which is a DataFrame that has the columns specified in the above\r\n","    section, \"Loading Datasets.\"\r\n","    \"\"\"\r\n","    self.model = model\r\n","    self.test_set = test_set\r\n","    # self.predictions will be a DataFrame of the same format as TEST_SET.\r\n","    # However, it is set to None here because it will be computed lazily.\r\n","    self.predictions = None\r\n","  def get_predictions(self):\r\n","    \"\"\"Returns a DataFrame with the same columns as the DataFrame on which the\r\n","    model was trained, as described in the \"Datasets\" section of this notebook.\r\n","    \"\"\"\r\n","    if self.predictions is None:\r\n","      self.predictions = self.model.predict(self.test_set.raw_text)\r\n","    return self.predictions\r\n","  def year_accuracy(self):\r\n","    \"\"\"Returns the proportion of publication years that the model predicts\r\n","    accurately.\r\n","    \"\"\"\r\n","    return np.mean(self.test_set.year == self.get_predictions().year)\r\n","  def surname_accuracy(self):\r\n","    \"\"\"Returns the proportion of citations from which the set of all authors'\r\n","    surnames is predicted with perfect accuracy (within the simplifications of\r\n","    de-accenting and capitalization/punctuation removal).\r\n","    \"\"\"\r\n","    return np.mean([\r\n","        (\r\n","            set(simple_preprocess(name.surname) for name in true_names)\r\n","            == set(simple_preprocess(name.surname) for name in predicted_names)\r\n","        ) for true_names, predicted_names\r\n","        in zip(self.test_set.author, self.get_predictions().author)\r\n","    ])\r\n","  def primary_author_surname_accuracy(self):\r\n","    \"\"\"Returns the proportion of citations from which the primary author's\r\n","    surname is predicted correctly. The first author who is listed in the\r\n","    dataset will be interpreted as the primary author.\r\n","    \"\"\"\r\n","    return np.mean([\r\n","        true_names[0].surname == predicted_names[0].surname\r\n","        for true_names, predicted_names\r\n","        in zip(self.test_set.author, self.get_predictions().author)\r\n","    ])\r\n","  def title_accuracy(self):\r\n","    \"\"\"Returns the proportion of citations from which the title of the cited work\r\n","    is predicted with perfect accuracy (within the simplifications of\r\n","    de-accenting and capitalization/punctuation removal).\r\n","    \"\"\"\r\n","    return np.mean(\r\n","        simple_preprocess(true_title) == simple_preprocess(predicted_title)\r\n","        for true_title, predicted_title\r\n","        in zip(self.test_set.title, self.get_predictions().title)\r\n","    )\r\n","  def quick_report(self):\r\n","    \"\"\"Prints a report of model performance without confidence intervals.\r\n","    Intended for development (debugging, preliminary results, etc.) and not for\r\n","    final reporting.\r\n","    \"\"\"\r\n","    t0 = time.time()\r\n","    get_predictions()\r\n","    elapsed_time = time.time() - t0\r\n","    print('Generated labels for {0:,} records in {:.4f} seconds ({:.4f} seconds'\r\n","          ' per record)'.format(\r\n","              len(self.test_set.index),\r\n","              elapsed_time,\r\n","              elapsed_time / len(self.test_set.index)))\r\n","    print('Surname accuracy (complete set): {:.4f}'.format(\r\n","        self.surname_accuracy()))\r\n","    print('Year accuracy: {:.4f}'.format(self.year_accuracy()))\r\n","    print('Title accuracy (with preproc): {:.4f}').format(self.title_accuracy())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eqx8G-vI7wii"},"source":["The following cell demonstrates the power of `simple_preprocess`, a simple, widely used utility whose function should be reasonably easy to explain to other people."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2pQPhdt32_rR","executionInfo":{"status":"ok","timestamp":1614319554470,"user_tz":480,"elapsed":304,"user":{"displayName":"Peter Donovan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmevrWa6lY-MoKL6lRCYdlS09IgSTYxUPNKCYZ=s64","userId":"01568505250649921698"}},"outputId":"4cfcbfbc-12aa-4ca0-d105-dc91efbe0f81"},"source":["simple_preprocess('The cát, in the hát.', deacc=True) == simple_preprocess('the cAt  iN tHe haT!!!?', deacc=True)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"bPviJEg5DY8P"},"source":["class CrossValidator:\r\n","  \"\"\"Reports cross validation results for a given model and dataset.\"\"\"\r\n","  # This should have a has-a relationship with TestRunner.\r\n","  # You know, it's possible sklearn has this. I will take a look."],"execution_count":null,"outputs":[]}]}